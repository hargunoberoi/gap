<!DOCTYPE html>
<html lang="en" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />
    <title>Understanding LLMs - Generative AI for Professionals</title>

    <!-- Bootstrap CSS -->
    <link
      rel="stylesheet"
      href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
    />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css"
    />


    <link
      rel="stylesheet"
      href="https://hargunoberoi.github.io/gap/theme/css/style.css"
    />

    <style>
      .navbar { background-color: #89CFF0 }
    </style>
  </head>
  <body>
    <nav class="navbar navbar-expand-md navbar-dark">
      <div class="container">
        <a class="navbar-brand" href="https://hargunoberoi.github.io/gap/">
          <img
            src="https://hargunoberoi.github.io/gap/images/logo.png"
            alt="Logo"
            class="mr-2"
            style="height: 40px; width: auto"
          />
        </a>
        <button
          class="navbar-toggler"
          type="button"
          data-toggle="collapse"
          data-target="#navbarDefault"
        >
          <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="navbarDefault">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item">
              <a class="nav-link" href="https://hargunoberoi.github.io/gap/./"
                >Home</a
              >
            </li>
            <li class="nav-item">
              <a class="nav-link" href="https://hargunoberoi.github.io/gap/pages/chapters.html"
                >Chapters</a
              >
            </li>
            <li class="nav-item">
              <a class="nav-link" href="https://hargunoberoi.github.io/gap/pages/materials.html"
                >Materials</a
              >
            </li>
            <li class="nav-item">
              <a class="nav-link" href="https://hargunoberoi.github.io/gap/pages/tools.html"
                >Tools</a
              >
            </li>
 
            <li class="nav-item">
              <button
                class="dark-mode-toggle"
                id="darkModeToggle"
                aria-label="Toggle dark mode"
                title="Toggle dark/light mode"
              >
                <i class="fas fa-moon"></i>
              </button>
            </li>
          </ul>
        </div>
      </div>
    </nav>

    <main id="content" class="container mt-4">
<article>
  <header>
    <h1>Understanding LLMs</h1>
    </div>
  </header>

  <div class="entry-content"><p><img alt="Understanding LLMs" src="https://github.com/hargunoberoi/gap/blob/main/docs/images/c1.png?raw=true"></p>
<h1>Chapter 1: Understanding LLMs</h1>
<h2>How Large Language Models are Trained</h2>
<p>Large Language Models (LLMs) represent a significant advancement in artificial intelligence, capable of understanding and generating human-like text across diverse domains.</p>
<h3>The Training Pipeline</h3>
<ol>
<li>
<p><strong>Data Collection and Preprocessing</strong></p>
</li>
<li>
<p>Web crawling and document extraction</p>
</li>
<li>Filtering for quality and removing harmful content</li>
<li>
<p>Tokenization of text into smaller units</p>
</li>
<li>
<p><strong>Pre-training Phase</strong></p>
</li>
<li>
<p>Self-supervised learning on massive text corpora</p>
</li>
<li>Next token prediction as the primary training objective</li>
<li>
<p>Distributed training across large GPU/TPU clusters</p>
</li>
<li>
<p><strong>Fine-tuning Phase</strong></p>
</li>
<li>Supervised fine-tuning (SFT) on curated datasets</li>
<li>Reinforcement Learning from Human Feedback (RLHF)</li>
<li>Alignment techniques to improve safety and helpfulness</li>
</ol>
<h2>Architecture and Scaling Principles</h2>
<p>LLMs are built upon the transformer architecture, with specific design choices that enable their remarkable capabilities.</p>
<h3>Model Components</h3>
<ul>
<li><strong>Token Embeddings</strong>: Converting tokens to vectors</li>
<li><strong>Positional Encodings</strong>: Providing sequence information</li>
<li><strong>Self-attention Layers</strong>: Capturing relationships between tokens</li>
<li><strong>Feed-forward Networks</strong>: Processing token representations</li>
<li><strong>Layer Normalization</strong>: Stabilizing activations</li>
</ul>
<h3>Scaling Laws</h3>
<p>LLM performance follows predictable scaling laws:</p>
<ul>
<li>Performance improves smoothly with model size (parameters)</li>
<li>More training data yields better results, following power laws</li>
<li>Compute-optimal scaling balances model size and training tokens</li>
</ul>
<h2>Data Collection and Training Methodologies</h2>
<p>The data used to train LLMs profoundly impacts their capabilities and limitations.</p>
<h3>Data Sources</h3>
<ul>
<li>Books and academic papers</li>
<li>Websites and online forums</li>
<li>Code repositories</li>
<li>Government and legal documents</li>
<li>Encyclopedia and reference materials</li>
</ul>
<h3>Training Challenges</h3>
<ul>
<li><strong>Computational Requirements</strong>: Training GPT-4 class models requires millions of GPU hours</li>
<li><strong>Data Quality Issues</strong>: Balancing quantity vs. quality</li>
<li><strong>Bias Mitigation</strong>: Addressing inherent biases in training data</li>
<li><strong>Catastrophic Forgetting</strong>: Maintaining performance across domains during fine-tuning</li>
</ul>
<h2>Hands-on: Experimenting with Foundation Models</h2>
<p>In this exercise, we'll work with different LLMs to understand their capabilities and limitations:</p>
<ol>
<li>
<p><strong>Comparing Model Responses</strong></p>
</li>
<li>
<p>Testing the same prompt across different model sizes and families</p>
</li>
<li>
<p>Analyzing variations in style, accuracy, and reasoning</p>
</li>
<li>
<p><strong>Language Capabilities</strong></p>
</li>
<li>
<p>Testing models on different languages</p>
</li>
<li>
<p>Exploring translation abilities</p>
</li>
<li>
<p><strong>Knowledge Boundaries</strong></p>
</li>
<li>Exploring knowledge cutoffs</li>
<li>Testing factual recall vs. reasoning</li>
</ol>
<h2>Key Takeaways</h2>
<ul>
<li>LLMs are trained through a multi-stage process requiring massive data and compute</li>
<li>Scaling laws show predictable improvements with more parameters and data</li>
<li>The quality and diversity of training data directly impact model capabilities</li>
<li>Different models excel at different tasks based on their training methodology</li>
</ul>
<h2>Further Reading</h2>
<ul>
<li>"Language Models are Few-Shot Learners" (GPT-3 paper)</li>
<li>"Training language models to follow instructions with human feedback" (RLHF paper)</li>
<li>"Scaling Laws for Neural Language Models" by Kaplan et al.</li>
</ul>
<h2>Next Chapter</h2>
<p>In <a href="chapter2.html">Chapter 2: Working with LLMs</a>, we'll explore practical techniques for prompt engineering and optimizing model outputs.</p></div>
</article>
    </main>

    <footer class="footer mt-5 py-3 bg-light">
      <div class="container">
        <span class="text-muted">Â© 2025 Hargun Singh Oberoi</span>
      </div>
    </footer>

    <!-- JavaScript -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>

    <!-- Dark Mode Script -->
    <script>
      (function () {
        // Execute immediately to avoid any delay
        const darkModeToggle = document.getElementById("darkModeToggle");
        const html = document.documentElement;

        // Check for saved theme preference or use default
        const savedTheme = localStorage.getItem("theme") || "light";

        // Apply theme immediately on page load
        html.setAttribute("data-theme", savedTheme);

        // Update icon based on current theme
        if (savedTheme === "dark") {
          darkModeToggle.innerHTML = '<i class="fas fa-sun"></i>';
        } else {
          darkModeToggle.innerHTML = '<i class="fas fa-moon"></i>';
        }

        // Add click handler
        darkModeToggle.addEventListener("click", function (e) {
          e.preventDefault();
          const currentTheme = html.getAttribute("data-theme");
          const newTheme = currentTheme === "dark" ? "light" : "dark";

          // Apply new theme
          html.setAttribute("data-theme", newTheme);
          localStorage.setItem("theme", newTheme);

          // Update icon
          this.innerHTML =
            newTheme === "dark"
              ? '<i class="fas fa-sun"></i>'
              : '<i class="fas fa-moon"></i>';

          console.log("Theme switched to:", newTheme); // Debug info
        });
      })();
    </script>

   </body>
</html>