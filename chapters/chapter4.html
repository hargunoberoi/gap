<!DOCTYPE html>
<html lang="en" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />
    <title>Multimodal LLMs - Generative AI for Professionals</title>

    <!-- Bootstrap CSS -->
    <link
      rel="stylesheet"
      href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
    />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css"
    />


    <link
      rel="stylesheet"
      href="https://hargunoberoi.github.io/gap/theme/css/style.css"
    />

    <style>
      .navbar { background-color: #89CFF0 }
    </style>
  </head>
  <body>
    <nav class="navbar navbar-expand-md navbar-dark">
      <div class="container">
        <a class="navbar-brand" href="https://hargunoberoi.github.io/gap/">
          <img
            src="https://hargunoberoi.github.io/gap/images/logo.png"
            alt="Logo"
            class="mr-2"
            style="height: 40px; width: auto"
          />
        </a>
        <button
          class="navbar-toggler"
          type="button"
          data-toggle="collapse"
          data-target="#navbarDefault"
        >
          <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="navbarDefault">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item">
              <a class="nav-link" href="https://hargunoberoi.github.io/gap/./"
                >Home</a
              >
            </li>
            <li class="nav-item">
              <a class="nav-link" href="https://hargunoberoi.github.io/gap/pages/chapters.html"
                >Chapters</a
              >
            </li>
            <li class="nav-item">
              <a class="nav-link" href="https://hargunoberoi.github.io/gap/pages/materials.html"
                >Material</a
              >
            </li>
            <li class="nav-item">
              <a class="nav-link" href="https://hargunoberoi.github.io/gap/pages/tools.html"
                >Tools</a
              >
            </li>
 
            <li class="nav-item">
              <button
                class="dark-mode-toggle"
                id="darkModeToggle"
                aria-label="Toggle dark mode"
                title="Toggle dark/light mode"
              >
                <i class="fas fa-moon"></i>
              </button>
            </li>
          </ul>
        </div>
      </div>
    </nav>

    <main id="content" class="container mt-4">
<article>
  <header>
    <h1>Multimodal LLMs</h1>
    </div>
  </header>

  <div class="entry-content"><p><img alt="Multimodal LLMs" src="https://github.com/hargunoberoi/gap/blob/main/docs/images/c4.png?raw=true"></p>
<h1>Chapter 4: Multimodal LLMs</h1>
<h2>Omni Models and Different Modalities</h2>
<p>The evolution from text-only to multimodal AI represents a significant leap in machine learning capabilities.</p>
<h3>What are Multimodal Models?</h3>
<ul>
<li>Systems that process and generate multiple types of data</li>
<li>Integration of text, images, audio, and video</li>
<li>Unified architectures that bridge different modalities</li>
</ul>
<h3>Key Multimodal Systems</h3>
<ul>
<li><strong>GPT-4V</strong>: OpenAI's vision-capable language model</li>
<li><strong>Claude Opus/Sonnet</strong>: Anthropic's multimodal models</li>
<li><strong>Gemini</strong>: Google's multimodal AI system</li>
<li><strong>LLaVA</strong>: Open-source multimodal frameworks</li>
</ul>
<h3>Cross-modal Understanding</h3>
<ul>
<li>Grounding language in visual perception</li>
<li>Describing visual concepts using natural language</li>
<li>Transferring knowledge between modalities</li>
</ul>
<h2>Diffusion Models for Image Generation</h2>
<p>Diffusion models have revolutionized AI image generation with unprecedented quality and control.</p>
<h3>How Diffusion Models Work</h3>
<ul>
<li>Start with noise and gradually remove it</li>
<li>Trained to reverse a noise-adding process</li>
<li>Balance between fidelity and diversity</li>
</ul>
<h3>Text-to-Image Systems</h3>
<ul>
<li><strong>DALL-E 3</strong>: OpenAI's advanced image generation model</li>
<li><strong>Midjourney</strong>: Specialized for artistic and creative outputs</li>
<li><strong>Stable Diffusion</strong>: Open-source image generation framework</li>
</ul>
<h3>Image Editing and Manipulation</h3>
<ul>
<li>Inpainting and outpainting techniques</li>
<li>Style transfer and artistic modifications</li>
<li>Concept-guided image editing</li>
</ul>
<h2>Text-to-Speech, Speech-to-Text, and Text-to-Video Technologies</h2>
<p>The expansion of generative AI to audio and video has created powerful new creative tools.</p>
<h3>Text-to-Speech Systems</h3>
<ul>
<li><strong>ElevenLabs</strong>: High-quality voice synthesis</li>
<li><strong>OpenAI's TTS</strong>: Natural-sounding speech generation</li>
<li>Voice cloning and personalization</li>
</ul>
<h3>Speech-to-Text Technologies</h3>
<ul>
<li><strong>Whisper</strong>: OpenAI's robust speech recognition model</li>
<li><strong>AssemblyAI</strong>: Real-time transcription services</li>
<li>Specialized applications for different accents and languages</li>
</ul>
<h3>Text-to-Video Generation</h3>
<ul>
<li><strong>Sora</strong>: OpenAI's text-to-video system</li>
<li><strong>Runway Gen-2</strong>: Creative video generation</li>
<li><strong>Heygen</strong>: AI-driven video creation platform</li>
</ul>
<h2>Hands-on: Creating and Editing AI-Generated Media</h2>
<p>In this exercise, we'll experiment with various multimodal generation and editing tasks:</p>
<ol>
<li>
<p><strong>Image Generation and Modification</strong></p>
</li>
<li>
<p>Creating images with specific styles and content</p>
</li>
<li>Editing existing images with AI assistance</li>
<li>
<p>Comparing different image generation systems</p>
</li>
<li>
<p><strong>Audio Generation and Processing</strong></p>
</li>
<li>
<p>Creating voice narrations with text-to-speech</p>
</li>
<li>Transcribing audio files using Whisper</li>
<li>
<p>Experimenting with voice cloning and styles</p>
</li>
<li>
<p><strong>Multimodal Applications</strong></p>
</li>
<li>Building a simple application combining text and images</li>
<li>Creating presentations with AI-generated visuals</li>
<li>Developing a basic multimedia content generator</li>
</ol>
<h2>Key Takeaways</h2>
<ul>
<li>Multimodal models enable seamless integration between different types of content</li>
<li>Diffusion models have transformed image generation and editing capabilities</li>
<li>Text-to-speech and speech-to-text technologies are approaching human-level quality</li>
<li>Video generation is emerging as the next frontier in generative AI</li>
<li>Ethical considerations become more complex with multimodal systems</li>
</ul>
<h2>Further Reading</h2>
<ul>
<li>"Diffusion Models Beat GANs on Image Synthesis" by Dhariwal and Nichol</li>
<li>"DALL-E 3: Improving Image Generation with Better Captions" by OpenAI</li>
<li>"LLaVA: Large Language and Vision Assistant" by Liu et al.</li>
</ul>
<h2>Next Chapter</h2>
<p>In <a href="chapter5.html">Chapter 5: Agents</a>, we'll explore how LLMs can be integrated into agentic frameworks that allow them to use tools and perform complex, multi-step tasks.</p></div>
</article>
    </main>

    <footer class="footer mt-5 py-3 bg-light">
      <div class="container">
        <span class="text-muted">Â© 2025 Hargun Singh Oberoi</span>
      </div>
    </footer>

    <!-- JavaScript -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>

    <!-- Dark Mode Script -->
    <script>
      (function () {
        // Execute immediately to avoid any delay
        const darkModeToggle = document.getElementById("darkModeToggle");
        const html = document.documentElement;

        // Check for saved theme preference or use default
        const savedTheme = localStorage.getItem("theme") || "light";

        // Apply theme immediately on page load
        html.setAttribute("data-theme", savedTheme);

        // Update icon based on current theme
        if (savedTheme === "dark") {
          darkModeToggle.innerHTML = '<i class="fas fa-sun"></i>';
        } else {
          darkModeToggle.innerHTML = '<i class="fas fa-moon"></i>';
        }

        // Add click handler
        darkModeToggle.addEventListener("click", function (e) {
          e.preventDefault();
          const currentTheme = html.getAttribute("data-theme");
          const newTheme = currentTheme === "dark" ? "light" : "dark";

          // Apply new theme
          html.setAttribute("data-theme", newTheme);
          localStorage.setItem("theme", newTheme);

          // Update icon
          this.innerHTML =
            newTheme === "dark"
              ? '<i class="fas fa-sun"></i>'
              : '<i class="fas fa-moon"></i>';

          console.log("Theme switched to:", newTheme); // Debug info
        });
      })();
    </script>

   </body>
</html>